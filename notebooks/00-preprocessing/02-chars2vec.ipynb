{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21f14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class Chars2Vec:\n",
    "\n",
    "    def __init__(self, emb_dim, char_to_ix):\n",
    "        '''\n",
    "        Creates chars2vec model.\n",
    "\n",
    "        :param emb_dim: int, dimension of embeddings.\n",
    "        :param char_to_ix: dict, keys are characters, values are sequence numbers of characters.\n",
    "        '''\n",
    "\n",
    "        if not isinstance(emb_dim, int) or emb_dim < 1:\n",
    "            raise TypeError(\"parameter 'emb_dim' must be a positive integer\")\n",
    "\n",
    "        if not isinstance(char_to_ix, dict):\n",
    "            raise TypeError(\"parameter 'char_to_ix' must be a dictionary\")\n",
    "\n",
    "        self.char_to_ix = char_to_ix\n",
    "        self.ix_to_char = {char_to_ix[ch]: ch for ch in char_to_ix}\n",
    "        self.vocab_size = len(self.char_to_ix)\n",
    "        self.dim = emb_dim\n",
    "        self.cache = {}\n",
    "\n",
    "        lstm_input = tf.keras.layers.Input(shape=(None, self.vocab_size))\n",
    "\n",
    "        x = tf.keras.layers.LSTM(emb_dim, return_sequences=True)(lstm_input)\n",
    "        x = tf.keras.layers.LSTM(emb_dim)(x)\n",
    "\n",
    "        self.embedding_model = tf.keras.models.Model(inputs=[lstm_input], outputs=x)\n",
    "\n",
    "        model_input_1 = tf.keras.layers.Input(shape=(None, self.vocab_size))\n",
    "        model_input_2 = tf.keras.layers.Input(shape=(None, self.vocab_size))\n",
    "\n",
    "        embedding_1 = self.embedding_model(model_input_1)\n",
    "        embedding_2 = self.embedding_model(model_input_2)\n",
    "        x = tf.keras.layers.Subtract()([embedding_1, embedding_2])\n",
    "        x = tf.keras.layers.Dot(1)([x, x])\n",
    "        model_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        self.model = tf.keras.models.Model(inputs=[model_input_1, model_input_2], outputs=model_output)\n",
    "        self.model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    def fit(self, word_pairs, targets,\n",
    "            max_epochs, patience, validation_split, batch_size):\n",
    "        '''\n",
    "        Fits model.\n",
    "\n",
    "        :param word_pairs: list or numpy.ndarray of word pairs.\n",
    "        :param targets: list or numpy.ndarray of targets.\n",
    "        :param max_epochs: parameter 'epochs' of tensorflow model.\n",
    "        :param patience: parameter 'patience' of callback in tensorflow model.\n",
    "        :param validation_split: parameter 'validation_split' of tensorflow model.\n",
    "        '''\n",
    "\n",
    "        word_pairs = np.array(word_pairs)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        if not isinstance(word_pairs, list) and not isinstance(word_pairs, np.ndarray):\n",
    "            raise TypeError(\"parameters 'word_pairs' must be a list or numpy.ndarray\")\n",
    "\n",
    "        if not isinstance(targets, list) and not isinstance(targets, np.ndarray):\n",
    "            raise TypeError(\"parameters 'targets' must be a list or numpy.ndarray\")\n",
    "\n",
    "        x_1, x_2 = [], []\n",
    "\n",
    "        for pair_words in word_pairs:\n",
    "            emb_list_1 = []\n",
    "            emb_list_2 = []\n",
    "\n",
    "            if not isinstance(pair_words[0], str) or not isinstance(pair_words[1], str):\n",
    "                raise TypeError(\"word must be a string\")\n",
    "\n",
    "            first_word = pair_words[0].lower()\n",
    "            second_word = pair_words[1].lower()\n",
    "\n",
    "            for t in range(len(first_word)):\n",
    "\n",
    "                if first_word[t] in self.char_to_ix:\n",
    "                    x = np.zeros(self.vocab_size)\n",
    "                    x[self.char_to_ix[first_word[t]]] = 1\n",
    "                    emb_list_1.append(x)\n",
    "\n",
    "                else:\n",
    "                    emb_list_1.append(np.zeros(self.vocab_size))\n",
    "\n",
    "            x_1.append(np.array(emb_list_1))\n",
    "\n",
    "            for t in range(len(second_word)):\n",
    "\n",
    "                if second_word[t] in self.char_to_ix:\n",
    "                    x = np.zeros(self.vocab_size)\n",
    "                    x[self.char_to_ix[second_word[t]]] = 1\n",
    "                    emb_list_2.append(x)\n",
    "\n",
    "                else:\n",
    "                    emb_list_2.append(np.zeros(self.vocab_size))\n",
    "\n",
    "            x_2.append(np.array(emb_list_2))\n",
    "\n",
    "        x_1_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(x_1)\n",
    "        x_2_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(x_2)\n",
    "        \n",
    "        self.model.fit([x_1_pad_seq, x_2_pad_seq], targets,\n",
    "                    batch_size=batch_size, epochs=max_epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)])\n",
    "\n",
    "    def vectorize_words(self, words, maxlen_padseq=None):\n",
    "        '''\n",
    "        Returns embeddings for list of words. Uses cache of word embeddings to vectorization speed up.\n",
    "\n",
    "        :param words: list or numpy.ndarray of strings.\n",
    "        :param maxlen_padseq: parameter 'maxlen' for tensorflow pad_sequences transform.\n",
    "\n",
    "        :return word_vectors: numpy.ndarray, word embeddings.\n",
    "        '''\n",
    "\n",
    "        if not isinstance(words, list) and not isinstance(words, np.ndarray):\n",
    "            raise TypeError(\"parameter 'words' must be a list or numpy.ndarray\")\n",
    "\n",
    "        words = [w.lower() for w in words]\n",
    "        unique_words = np.unique(words)\n",
    "        new_words = [w for w in unique_words if w not in self.cache]\n",
    "\n",
    "        if len(new_words) > 0:\n",
    "\n",
    "            list_of_embeddings = []\n",
    "\n",
    "            for current_word in new_words:\n",
    "\n",
    "                if not isinstance(current_word, str):\n",
    "                    raise TypeError(\"word must be a string\")\n",
    "\n",
    "                current_embedding = []\n",
    "\n",
    "                for t in range(len(current_word)):\n",
    "\n",
    "                    if current_word[t] in self.char_to_ix:\n",
    "                        x = np.zeros(self.vocab_size)\n",
    "                        x[self.char_to_ix[current_word[t]]] = 1\n",
    "                        current_embedding.append(x)\n",
    "\n",
    "                    else:\n",
    "                        current_embedding.append(np.zeros(self.vocab_size))\n",
    "\n",
    "                list_of_embeddings.append(np.array(current_embedding))\n",
    "\n",
    "            embeddings_pad_seq = tf.keras.preprocessing.sequence.pad_sequences(list_of_embeddings, maxlen=maxlen_padseq)\n",
    "            new_words_vectors = self.embedding_model(embeddings_pad_seq)\n",
    "\n",
    "            for i in range(len(new_words)):\n",
    "                self.cache[new_words[i]] = new_words_vectors[i]\n",
    "\n",
    "        word_vectors = [self.cache[current_word] for current_word in words]\n",
    "\n",
    "        return np.array(word_vectors)\n",
    "\n",
    "def save_model(c2v_model, path_to_model):\n",
    "    '''\n",
    "    Saves trained model to directory.\n",
    "\n",
    "    :param c2v_model: Chars2Vec object, trained model.\n",
    "    :param path_to_model: str, path to save model.\n",
    "    '''\n",
    "\n",
    "    if not os.path.exists(path_to_model):\n",
    "        os.makedirs(path_to_model)\n",
    "\n",
    "    c2v_model.embedding_model.save_weights(path_to_model + '/weights.h5')\n",
    "\n",
    "    with open(path_to_model + '/model.pkl', 'wb') as f:\n",
    "        pickle.dump([c2v_model.dim, c2v_model.char_to_ix], f, protocol=2)\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    '''\n",
    "    Loads trained model.\n",
    "\n",
    "    :param path: str, if it is 'eng_50', 'eng_100', 'eng_150', 'eng_200' or 'eng_300' then loads one of default models,\n",
    "     else loads model from `path`.\n",
    "\n",
    "    :return c2v_model: Chars2Vec object, trained model.\n",
    "    '''\n",
    "\n",
    "    if path in ['eng_50', 'eng_100', 'eng_150', 'eng_200', 'eng_300']:\n",
    "        path_to_model = os.path.dirname(os.path.abspath(__file__)) + '/trained_models/' + path\n",
    "\n",
    "    else:\n",
    "        path_to_model = path\n",
    "\n",
    "    with open(path_to_model + '/model.pkl', 'rb') as f:\n",
    "        structure = pickle.load(f)\n",
    "        emb_dim, char_to_ix = structure[0], structure[1]\n",
    "\n",
    "    c2v_model = Chars2Vec(emb_dim, char_to_ix)\n",
    "    c2v_model.embedding_model.load_weights(path_to_model + '/weights.h5')\n",
    "    c2v_model.embedding_model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return c2v_model\n",
    "\n",
    "\n",
    "def train_model(emb_dim, X_train, y_train, model_chars,\n",
    "                max_epochs=200, patience=10, validation_split=0.05, batch_size=64):\n",
    "    '''\n",
    "    Creates and trains chars2vec model using given training data.\n",
    "\n",
    "    :param emb_dim: int, dimension of embeddings.\n",
    "    :param X_train: list or numpy.ndarray of word pairs.\n",
    "    :param y_train: list or numpy.ndarray of target values that describe the proximity of words.\n",
    "    :param model_chars: list or numpy.ndarray of basic chars in model.\n",
    "    :param max_epochs: parameter 'epochs' of keras model.\n",
    "    :param patience: parameter 'patience' of callback in keras model.\n",
    "    :param validation_split: parameter 'validation_split' of keras model.\n",
    "    :param batch_size: parameter 'batch_size' of keras model.\n",
    "\n",
    "    :return c2v_model: Chars2Vec object, trained model.\n",
    "    '''\n",
    "\n",
    "    if not isinstance(X_train, list) and not isinstance(X_train, np.ndarray):\n",
    "        raise TypeError(\"parameter 'X_train' must be a list or numpy.ndarray\")\\\n",
    "\n",
    "    if not isinstance(y_train, list) and not isinstance(y_train, np.ndarray):\n",
    "        raise TypeError(\"parameter 'y_train' must be a list or numpy.ndarray\")\n",
    "\n",
    "    if not isinstance(model_chars, list) and not isinstance(model_chars, np.ndarray):\n",
    "        raise TypeError(\"parameter 'model_chars' must be a list or numpy.ndarray\")\n",
    "\n",
    "    char_to_ix = {ch: i for i, ch in enumerate(model_chars)}\n",
    "    c2v_model = Chars2Vec(emb_dim, char_to_ix)\n",
    "\n",
    "    targets = [float(el) for el in y_train]\n",
    "    c2v_model.fit(X_train, targets, max_epochs, patience, validation_split, batch_size)\n",
    "\n",
    "    return c2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb68782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "\n",
    "path_to_model = '.'\n",
    "\n",
    "X_train = [('mecbanizing', 'mechanizing'), # similar words, target is equal 0\n",
    "           ('dicovery', 'dis7overy'), # similar words, target is equal 0\n",
    "           ('prot$oplasmatic', 'prtoplasmatic'), # similar words, target is equal 0\n",
    "           ('copulateng', 'lzateful'), # not similar words, target is equal 1\n",
    "           ('estry', 'evadin6'), # not similar words, target is equal 1\n",
    "           ('cirrfosis', 'afear') # not similar words, target is equal 1\n",
    "          ]\n",
    "\n",
    "y_train = [0, 0, 0, 1, 1, 1]\n",
    "\n",
    "model_chars = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.',\n",
    "               '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<',\n",
    "               '=', '>', '?', '@', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
    "               'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
    "               'x', 'y', 'z']\n",
    "\n",
    "# Create and train chars2vec model using given training data\n",
    "my_c2v_model = train_model(dim, X_train, y_train, model_chars)\n",
    "\n",
    "# Save pretrained model\n",
    "save_model(my_c2v_model, path_to_model)\n",
    "\n",
    "words = ['list', 'of', 'words']\n",
    "\n",
    "# Load pretrained model, create word embeddings\n",
    "c2v_model = load_model(path_to_model)\n",
    "word_embeddings = c2v_model.vectorize_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca19e744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00643387,  0.01794295, -0.01599562,  0.01166679,  0.00539388,\n",
       "         0.00768752, -0.01599908,  0.00968522,  0.00172485,  0.00965775,\n",
       "        -0.00617322,  0.00083872, -0.02359655,  0.0037647 ,  0.00461224,\n",
       "        -0.00135553,  0.01926542,  0.00423662, -0.01475451,  0.02239291,\n",
       "         0.03318596,  0.01423749,  0.01145388,  0.01241648,  0.02299693,\n",
       "         0.00012665,  0.01425277,  0.00637047,  0.02074144, -0.01051522,\n",
       "         0.0133602 ,  0.00527671, -0.00858078,  0.01262032, -0.0176644 ,\n",
       "        -0.02334343, -0.0221853 ,  0.00539844,  0.01823559,  0.00431189,\n",
       "        -0.00346982, -0.013614  ,  0.00717476, -0.00369649, -0.00607695,\n",
       "         0.00913777,  0.0153642 ,  0.00129303, -0.00808067, -0.0278762 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2v_model.vectorize_words(['homer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a73a72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00654842,  0.01498611, -0.01493716,  0.00941052,  0.00300757,\n",
       "         0.00774431, -0.01313346,  0.01279095,  0.00662093,  0.01282843,\n",
       "        -0.00416017, -0.00337917, -0.01472683,  0.00380855,  0.00639721,\n",
       "         0.0035616 ,  0.01929864,  0.00279975, -0.00771315,  0.01808963,\n",
       "         0.0222703 ,  0.01078383,  0.00834743,  0.0166842 ,  0.01898005,\n",
       "         0.00909053,  0.00919381,  0.00101805,  0.01332508, -0.0132224 ,\n",
       "         0.01025588, -0.00020884, -0.01025653,  0.01442861, -0.01744916,\n",
       "        -0.01730256, -0.01981951,  0.00361774,  0.01308391, -0.00541571,\n",
       "         0.00056039, -0.01087464,  0.00636775, -0.00070609, -0.00558699,\n",
       "         0.0068334 ,  0.01625771,  0.00344713, -0.00554192, -0.0182513 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2v_model.vectorize_words(['home'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (entity_name_matching_dnn)",
   "language": "python",
   "name": "kedro_entity_name_matching_dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
